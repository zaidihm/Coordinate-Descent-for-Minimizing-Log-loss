# Logistic Regression Optimization with Coordinate Descent

This project demonstrates the implementation and comparison of gradient descent and coordinate descent algorithms to optimize logistic regression for binary classification. The project includes detailed methodology, mathematical formulations, and visualizations.

## Table of Contents
- [Introduction](#introduction)
- [Methodology](#methodology)
- [Feature Engineering](#feature-engineering)
- [Train Test Split and Standardization](#train-test-split-and-standardization)
- [Implementing the Algorithms](#implementing-the-algorithms)
- [Comparison and Visualization](#comparison-and-visualization)
- [Usage](#usage)
- [Results](#results)
- [Contributing](#contributing)

## Introduction
This project is part of the Complex Engineering Problems (CEP) for the DS221 Inferential Statistics course at the GIK Institute, taught by Dr. Babar Zaman. The goal is to optimize the logistic regression model using both gradient descent and coordinate descent algorithms.

## Methodology
The cross-entropy loss function \( L(w) \) is minimized using gradient and coordinate descent methods. The mathematical formulations and gradients are defined as follows:

![Screenshot 2024-05-14 172941](https://github.com/0xnomy/Coordinate-Descent-for-Minimizing-Log-Loss/assets/63780923/d8f28781-ef05-4239-b75c-b6bdb06b1ff9)


## Feature Engineering
- Dropped irrelevant columns and separated the target variable.
- Converted categorical features to dummy variables.

## Train Test Split and Standardization
- Split the dataset into training and testing sets.
- Standardized the features for better performance.

## Implementing the Algorithms
Implemented both gradient descent and coordinate descent algorithms to minimize the log-loss.

## Comparison and Visualization
Compared the performance of gradient descent, coordinate descent, and scikit-learn's Logistic Regression. Visualized the loss curves for both algorithms.

## Usage
Clone the repository and run the Jupyter Notebook to see the implementation details and visualizations:

```sh
git clone https://github.com/yourusername/Logistic-Regression-Optimization.git
cd Logistic-Regression-Optimization
jupyter notebook
```

## Results
- Initial Log-Loss with zero weights.
- Final Log-Loss after Gradient Descent.
- Final Log-Loss after Coordinate Descent.
- Logistic Regression Log-Loss (scikit-learn).

## Contributing
If you find any errors or have suggestions for improvements, please feel free to fork the repository and submit a pull request or comment on the issues.

---

**Note:** The LaTeX code and mathematical formulations were generated by Gemini. If you like this project, please give it an upvote. Also, connect with me if you're interested in data science, machine learning, or AI!

---

Connect with me on [LinkedIn](https://www.linkedin.com/in/naumanalimurad)

---

Feel free to adjust any details and add any specific links, images, or badges as needed!
